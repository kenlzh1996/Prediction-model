{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f79d054",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a24e3",
   "metadata": {},
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7c17f9-bca3-44e5-b665-942a8cc73975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "from pdb import set_trace\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73991f82",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0515987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the GPU is available and store it as a variable so tensors can be moved to it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print('Is cuda available?:', torch.cuda.is_available())\n",
    "print('cuda version:', torch.version.cuda)\n",
    "dev = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d080d96-3db7-4b7a-8e17-3c711e919f96",
   "metadata": {},
   "source": [
    "### Variables and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03e9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamp for the summary file name\n",
    "Time_Stamp = datetime.datetime.now().strftime(\"%Y_%m%d%_H%M\")\n",
    "\n",
    "# Create run name\n",
    "Run_Name = 'Dec 27; PixelPreserve; customstride2' # change run_name before running\n",
    "\n",
    "# Paths\n",
    "dataset_folder_path = '/home/ec2-user/MLNotebook shared/Datasets/' # dataset folder with all the proteome and mRNA data\n",
    "summary_folder_path = '/home/ec2-user/MLNotebook shared/Summary/' # summary folder path to store summary reports\n",
    "model_folder_path = '/home/ec2-user/MLNotebook shared/Model exports/' # model folder path to store models\n",
    "figure_folder_path = '/home/ec2-user/MLNotebook shared/Figure exports/' # figure folder path to store figures\n",
    "performance_folder_path = '/home/ec2-user/MLNotebook shared/Performance DF exports/' # performance folder path to store performance dataframes\n",
    "\n",
    "# Define the Model\n",
    "Model = 'CustomNN-stride2' # 'ResNet18', 'CustomNN-reg', and 'CustomNN-stride2' are the options\n",
    "\n",
    "# Define loss function\n",
    "LossFunc_Name = 'CrossEntropy' # 'CrossEntropy', 'Focal' are the options\n",
    "\n",
    "# Define the optimizer\n",
    "Optim_Name = 'AdamW' # 'Adam', 'AdamW', 'SGD', 'SGD_weight' are the options\n",
    "\n",
    "# Dataset\n",
    "Set = 'Protein + mRNA' # Define the dataset to use, 'Protein', 'mRNA', or 'Protein + mRNA' are the options\n",
    "\n",
    "# Image size\n",
    "Canvas_Size = 18\n",
    "\n",
    "# Seed number\n",
    "Seed = 43\n",
    "\n",
    "# Validation fraction\n",
    "Validation_Fraction = 0.15\n",
    "\n",
    "# Standard deviation for add-noise transformation\n",
    "StandardDeviation = 0.05\n",
    "\n",
    "# Learning rate scheduler\n",
    "LrScheduler = True\n",
    "Learner_rate = 5e-5\n",
    "\n",
    "# Training set imbalance sampling method, can only choose one\n",
    "ImbalanceSampler = False\n",
    "\n",
    "# Optimizer\n",
    "Weight_Decay = 0.0001 # L2 regulator\n",
    "#  it works by adding a penalty to the loss function, which discourages large weights in the model; penalizes for too many weights - helps prevent overfitting\n",
    "#  the penalty is calculated as weight_decay * weight^2, and it's added to the loss.\n",
    "#  1e-4 or 1e-3 gives higher acc\n",
    "\n",
    "Momentum = 0.9\n",
    "#  specific for SGD optimizer, not applicable when using Adam optimizer\n",
    "#  a way to smooth noise that is passed to the optimizer, 0.9\n",
    "#  momentum is deterimental without label smoothing\n",
    "\n",
    "# Loss Function (CrossEntropyLoss)\n",
    "Label_Smoothing = 0.05\n",
    "#  sets the target of the loss function to something greater than 0 and less than 1\n",
    "#  helps prevent overfitting\n",
    "\n",
    "# Early stopping\n",
    "Earlystopping = True\n",
    "\n",
    "# Transformation\n",
    "TransformOrNot = True\n",
    "\n",
    "# Class weight to loss function\n",
    "ApplyClassWeightToLoss = True\n",
    "#  makes the loss weights equal to the fraction of each category label\n",
    "#  note, a layer is added to the model so the outputs of the model are equal to the number of categories\n",
    "\n",
    "# Batch size\n",
    "batch = 64\n",
    "\n",
    "# Epoch amount\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcb615",
   "metadata": {},
   "source": [
    "### Protein, mRNA dataframe, localization label set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af90684-fea6-47b5-8079-0dd01492c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the proteomics data and only keep genes (rows) that are fully quantified\n",
    "if Set == 'Protein':\n",
    "    PFP = 'K_single_PD_synthetic_kde.csv' # synthetic data\n",
    "elif Set == 'Protein + mRNA':\n",
    "    PFP = 'K_dual_PD_synthetic_kde.csv' # synthetic data\n",
    "else:\n",
    "    PFP = None\n",
    "\n",
    "if PFP is not None:\n",
    "    PD = pd.read_csv(dataset_folder_path+PFP)\n",
    "\n",
    "    # Data set wrangling\n",
    "    PD.index = PD.loc[:,'Index']\n",
    "    PD = PD.loc[:,PD.columns!='Index']\n",
    "    PD.dropna(inplace=True)\n",
    "\n",
    "    # # Put values of each column in the DataFrame into a list\n",
    "    # values = np.sort(PD.values.flatten().tolist())\n",
    "\n",
    "    # #Find the 2.5 and 97.5 percentile\n",
    "    # percentile_high = np.percentile(values, 97.5)\n",
    "    # percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "    # # Use the percentile for normalization\n",
    "    # PD = (PD - percentile_low) / (percentile_high - percentile_low)\n",
    "\n",
    "# Open the mRNA data and only keep genes (rows) that are fully quantified\n",
    "if Set == 'mRNA':\n",
    "    MFP = 'K_single_MD_synthetic_kde.csv' # mRNA file path, gene centric median normalized, log2 transformed\n",
    "elif Set == 'Protein + mRNA':\n",
    "    MFP = 'K_dual_MD_synthetic_kde.csv'\n",
    "else:\n",
    "    MFP = None\n",
    "\n",
    "if MFP is not None:\n",
    "    MD = pd.read_csv(dataset_folder_path + MFP)\n",
    "\n",
    "    # Data set wrangling\n",
    "    MD.index = MD.loc[:,'gene_name']\n",
    "    MD = MD.loc[:,MD.columns!='gene_name']\n",
    "    MD = MD.drop_duplicates()\n",
    "    MD.dropna(inplace=True)\n",
    "\n",
    "    # # Put values of each column in the DataFrame into a list\n",
    "    # values = np.sort(MD.values.flatten().tolist())\n",
    "\n",
    "    # # Find the 2.5 and 97.5 percentile\n",
    "    # percentile_high = np.percentile(values, 97.5)\n",
    "    # percentile_low = np.percentile(values, 2.5)\n",
    "\n",
    "    # # Use the percentile for normalization\n",
    "    # MD = (MD - percentile_low) / (percentile_high - percentile_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d093503b-cbc4-47ee-a7a6-3d9f3b371687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the labels data\n",
    "LFP = 'SubCellBarcode.MCF7.txt'\n",
    "LD = pd.read_csv(filepath_or_buffer=dataset_folder_path+LFP,sep='\\t')\n",
    "\n",
    "# Data set wrangling\n",
    "LD.index = LD.loc[:,'Protein']\n",
    "LD = LD.loc[:,LD.columns!='Protein']\n",
    "\n",
    "# Remove unclassified class\n",
    "NotUnclassInd = LD.loc[:,'Localization'] != 'Unclassified'\n",
    "LD = LD.loc[NotUnclassInd,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b73e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Set == 'mRNA':\n",
    "    LD = LD.loc[MD.index,:]\n",
    "elif Set == 'Protein':\n",
    "    LD = LD.loc[PD.index,:]\n",
    "elif Set == 'Protein + mRNA':\n",
    "    LD = LD.loc[PD.index,:]\n",
    "\n",
    "print('Number of labels in Krug dataset')\n",
    "print(len(LD.index))\n",
    "print(LD['Localization'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfb7cc",
   "metadata": {},
   "source": [
    "### Set seed function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688d65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed) # set random seed for python\n",
    "    np.random.seed(seed) # set random seed for numpy\n",
    "    torch.manual_seed(seed) # set random seed for CPU\n",
    "    rs = RandomState(MT19937(SeedSequence(seed))) # seed for numpy\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed) # set random seed for all GPUs\n",
    "    torch.backends.cudnn.deterministic = True # set to True to get reproducible results\n",
    "    torch.backends.cudnn.benchmark = False # set to False to get reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ff743",
   "metadata": {},
   "source": [
    "### Canvas and RGB tensor generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d321c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave_arrays(set=None, PD=None, MD=None):\n",
    "    \"\"\"\n",
    "    Interleave the values from PD, MD and RD arrays if using both datasets and feature engineering, otherwise sort the PD or MD array.\n",
    "    \n",
    "    Parameters:\n",
    "    set (str): Dataset used for plotting ('Protein', 'mRNA', 'Protein + mRNA').\n",
    "    PD (pd.DataFrame): Array containing normalized protein abundance data.\n",
    "    MD (pd.DataFrame): Array containing normalized mRNA abundance data.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Interleaved array of PD and MD values if both are provided, otherwise sorted PD or MD array.\n",
    "    \"\"\"\n",
    "    def sort_and_interleave(PD, MD=None):\n",
    "        # Convert DataFrames to numpy arrays if necessary\n",
    "        PD_array = PD.values if isinstance(PD, pd.DataFrame) else PD\n",
    "        MD_array = MD.values if isinstance(MD, pd.DataFrame) else MD\n",
    "\n",
    "        # Ensure PD and MD have compatible shapes\n",
    "        assert PD_array.shape == MD_array.shape if MD_array is not None else True, \\\n",
    "            \"PD and MD must have the same shape if both are provided.\"\n",
    "\n",
    "        # Compute the sum for sorting if MD is provided, otherwise just use PD\n",
    "        sum_abundance = PD_array + (MD_array if MD_array is not None else 0)\n",
    "\n",
    "        # Sort indices by sum_abundance for each row\n",
    "        sorted_indices = np.argsort(sum_abundance, axis=1)  # Ascending order\n",
    "\n",
    "        # Sort PD and MD based on sorted indices\n",
    "        try:\n",
    "            PD_sorted = np.take_along_axis(PD_array, sorted_indices, axis=1)\n",
    "            if MD_array is not None:\n",
    "                MD_sorted = np.take_along_axis(MD_array, sorted_indices, axis=1)\n",
    "                return PD_sorted, MD_sorted\n",
    "            else:\n",
    "                return PD_sorted, None\n",
    "        except IndexError as e:\n",
    "            print(\"IndexError in sort_and_interleave:\")\n",
    "            print(f\"PD_array shape: {PD_array.shape}, MD_array shape: {MD_array.shape if MD_array is not None else 'None'}\")\n",
    "            print(f\"sorted_indices shape: {sorted_indices.shape}\")\n",
    "            raise e\n",
    "\n",
    "    if set == 'Protein + mRNA':\n",
    "        assert PD.shape == MD.shape, \"PD and MD dataframes must have the same shape.\"\n",
    "        PD_sorted, MD_sorted = sort_and_interleave(PD, MD)\n",
    "        \n",
    "        interleaved_array = np.empty((len(PD_sorted), Canvas_Size*Canvas_Size), dtype=PD_sorted.dtype)\n",
    "        interleaved_array[:, 0::2] = PD_sorted\n",
    "        interleaved_array[:, 1::2] = MD_sorted\n",
    "        return interleaved_array\n",
    "\n",
    "    elif set == 'Protein':\n",
    "        assert PD is not None and MD is None, \"PD dataframe must be provided.\"\n",
    "        PD_sorted, _ = sort_and_interleave(PD)\n",
    "        return PD_sorted\n",
    "\n",
    "    elif set == 'mRNA':\n",
    "        assert MD is not None and PD is None, \"MD dataframe must be provided.\"\n",
    "        MD_sorted, _ = sort_and_interleave(MD)\n",
    "        return MD_sorted\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Set must be 'Protein', 'mRNA', or 'Protein + mRNA'\")\n",
    "\n",
    "def gen_horizontal_coords(n, num_points):\n",
    "    \"\"\"\n",
    "    Generate coordinates for a horizontal pattern starting from the top-left corner.\n",
    "\n",
    "    Parameters:\n",
    "    n (int): Size of the canvas (n x n).\n",
    "    num_points (int): Number of points to generate in the horizontal pattern.\n",
    "\n",
    "    Returns:\n",
    "    list: List of (x, y) coordinates in horizontal order.\n",
    "    \"\"\"\n",
    "    coords = [(x, y) for x in range(n) for y in range(n)]\n",
    "    return coords[:num_points]\n",
    "\n",
    "def create_rgb_tensors(set, PD, MD):\n",
    "    \"\"\"\n",
    "    Create RGB tensors from PD and MD data using the specified pattern.\n",
    "    \n",
    "    Parameters:\n",
    "    PD (pd.DataFrame): DataFrame containing normalized protein abundance data.\n",
    "    MD (pd.DataFrame): DataFrame containing normalized mRNA abundance data.\n",
    "    pattern (str): Pattern to use for arranging the data ('spiral').\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: RGB tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    final_array = interleave_arrays(set, PD, MD)\n",
    "    final_array = final_array.flatten()\n",
    "    num_pixels = len(final_array)\n",
    "    \n",
    "    # Generate coordinates based on the pattern, spiral or horizontal arrangment\n",
    "    coords = gen_horizontal_coords(Canvas_Size, num_pixels)\n",
    "\n",
    "    # Create RGB tensor\n",
    "    rgb_tensor = np.zeros((Canvas_Size, Canvas_Size, 3), dtype=np.float32)\n",
    "    for i, (x, y) in enumerate(coords):\n",
    "        if i < len(final_array):\n",
    "            value = final_array[i]\n",
    "            scaled_val = (value - (-4))/8\n",
    "            # Assign colors based on the value, if abundance > 1, set it to red, if abundance < 0, set it to blue.\n",
    "            if value > 4:\n",
    "                rgb_tensor[x, y] = [255, 0, 0]  # Red for value > 1\n",
    "            elif value < -4:\n",
    "                rgb_tensor[x, y] = [0, 0, 255]  # Blue for value < 0\n",
    "            else:\n",
    "                rgb_tensor[x, y] = [255 * scaled_val, 0, 255 * (1 - scaled_val)]\n",
    "\n",
    "    return torch.from_numpy(rgb_tensor.transpose(2, 0, 1))  # Convert to CHW format for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f7580",
   "metadata": {},
   "source": [
    "### Transformation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aafb40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseTransform:\n",
    "    \"\"\"\n",
    "    A class that applies normal noise transformation to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        mean (float): The mean of the normal distribution. Default is 0.\n",
    "        std_dev (float): The standard deviation of the normal distribution. Default is 0.2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean=0, std_dev=StandardDeviation):\n",
    "        super(NoiseTransform, self).__init__()\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_dev\n",
    "\n",
    "    def forward(self, df):\n",
    "        \"\"\"\n",
    "        Applies normal noise transformation to the input DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The transformed DataFrame with normal noise applied.\n",
    "        \"\"\"\n",
    "        tensor = torch.from_numpy(df.values).float()\n",
    "        noise = torch.empty(tensor.size()).normal_(self.mean, self.std_dev)\n",
    "        noisy_tensor = tensor + noise\n",
    "        #noisy_tensor = noisy_tensor.clamp(0, 1) # Clamp the values of noisy_tensor between 0 and 1\n",
    "        \n",
    "        # Convert the tensor back to DataFrame\n",
    "        df_transformed = pd.DataFrame(noisy_tensor.numpy(), index=df.index, columns=df.columns)\n",
    "        \n",
    "        return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517c7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedTransform:\n",
    "    \"\"\"\n",
    "    A class representing a combined transformation.\n",
    "\n",
    "    Args:\n",
    "        set: The dataset used for the transformation.\n",
    "        transform1: The first transformation to apply.\n",
    "        transform2: The second transformation to apply (optional).\n",
    "        FeatureEngineering: Whether to use the feature engineering data.\n",
    "\n",
    "    Attributes:\n",
    "        transform1: The first transformation.\n",
    "        transform2: The second transformation.\n",
    "        FeatureEngineering: Whether to use the feature engineering data.\n",
    "\n",
    "    Methods:\n",
    "        __call__: Applies the combined transformation to the input dataframes.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, set, transform1=None, transform2=None):\n",
    "        self.set = set\n",
    "        self.transform1 = transform1 if transform1 is not None else None\n",
    "        self.transform2 = transform2 if transform2 is not None else None\n",
    "\n",
    "    def __call__(self, set, df1, df2):\n",
    "        \"\"\"\n",
    "        Applies the combined transformation to the input dataframes.\n",
    "\n",
    "        Args:\n",
    "            set: The dataset used for the transformation.\n",
    "            df1: The first dataframe.\n",
    "            df2: The second dataframe.\n",
    "\n",
    "        Returns:\n",
    "            transformed_rgb_tensors: The transformed RGB tensors.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # apply the first transformation (noise) to df1 and df2; df1 is PD, df2 is MD\n",
    "        if self.transform1 is not None:\n",
    "            df1 = self.transform1.forward(df1) if df1 is not None else None\n",
    "            df2 = self.transform1.forward(df2) if df2 is not None else None\n",
    "            \n",
    "        # apply the second transformation (random column) to df1 and df2\n",
    "        if self.transform2 is not None:\n",
    "            df1, df2 = self.transform2.forward(df1, df2) if df1 is not None and df2 is not None else (None, None)\n",
    "        \n",
    "        # if we are using feature engineering, derive third dataframe from already noise and/or column random transformed \n",
    "        if self.set == 'Protein':\n",
    "            transformed_rgb_tensors = create_rgb_tensors(self.set, df1, None)\n",
    "        elif self.set == 'mRNA':\n",
    "            transformed_rgb_tensors = create_rgb_tensors(self.set, None, df2)\n",
    "        elif self.set == 'Protein + mRNA':\n",
    "            transformed_rgb_tensors = create_rgb_tensors(self.set, df1, df2)\n",
    "        \n",
    "        if transformed_rgb_tensors is None:\n",
    "            raise ValueError(\"Set must be 'Protein', 'mRNA', or 'Protein + mRNA'\")\n",
    "                \n",
    "        return transformed_rgb_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee71e5",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03cc632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, set, PD, MD, labels, transform=None):\n",
    "        self.set = set\n",
    "        self.PD = PD if PD is not None else None\n",
    "        self.MD = MD if MD is not None else None\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pd_data = self.PD.iloc[idx].to_frame().T if self.PD is not None else None\n",
    "        md_data = self.MD.iloc[idx].to_frame().T if self.MD is not None else None\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:  \n",
    "            if self.set == 'Protein + mRNA':\n",
    "                tensor = self.transform(self.set, pd_data, md_data)\n",
    "            elif self.set == 'Protein':\n",
    "                tensor = self.transform(self.set, pd_data, None)\n",
    "            elif self.set == 'mRNA':\n",
    "                tensor = self.transform(self.set, None, md_data)\n",
    "\n",
    "        else:\n",
    "            if self.set == 'Protein + mRNA':\n",
    "                tensor = create_rgb_tensors(self.set, pd_data, md_data)\n",
    "            elif self.set == 'Protein':\n",
    "                tensor = create_rgb_tensors(self.set, pd_data, None)\n",
    "            elif self.set == 'mRNA':\n",
    "                tensor = create_rgb_tensors(self.set, None, md_data)\n",
    "\n",
    "        # Ensure tensor is not None\n",
    "        if tensor is None:\n",
    "            raise ValueError(\"Both PD and MD are None, cannot create tensor.\")\n",
    "\n",
    "        return tensor, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e719b39",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9cfd9",
   "metadata": {},
   "source": [
    "### Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f7ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transform\n",
    "noise_transform = NoiseTransform()\n",
    "\n",
    "# Create an instance of the combined transformï¼Œ change here if doing different transformation\n",
    "# Transform 1 default is noise_transform, transform 2 default is TBD\n",
    "transform = CombinedTransform(set=Set, transform1=noise_transform, transform2=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b651e",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4713f80-0717-4831-923e-4a2aa9949dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed to ensure reproducibility\n",
    "set_seed(Seed)\n",
    "\n",
    "# Create a LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the labels to integers from LD, the label dataframe\n",
    "labels = encoder.fit_transform(LD.values.ravel())\n",
    "\n",
    "# Convert the labels to tensors\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Split the data into training and validation sets using the labels indices, random state is set\n",
    "indices = range(len(labels))\n",
    "train_indices, val_indices, train_labels, val_labels = train_test_split(indices, labels, test_size=Validation_Fraction, random_state=42)\n",
    "\n",
    "# Use the indices to split PD and MD\n",
    "if PFP is not None:\n",
    "    if Set == 'Protein' or 'Protein + mRNA':\n",
    "        train_PD = PD.iloc[train_indices]\n",
    "        val_PD = PD.iloc[val_indices]\n",
    "        print('Proteome data set is splited')\n",
    "if MFP is not None:\n",
    "    if Set == 'mRNA' or 'Protein + mRNA':\n",
    "        train_MD = MD.iloc[train_indices]\n",
    "        val_MD = MD.iloc[val_indices]\n",
    "        print('mRNA data set is splited')\n",
    "\n",
    "## Create the datasets\n",
    "# Initialize transform to None\n",
    "transform_to_use = None\n",
    "\n",
    "# Check if transformations should be applied\n",
    "if TransformOrNot:\n",
    "    transform_to_use = transform # transform is defined above\n",
    "\n",
    "# Check Dataset argument to determine which dataset to use\n",
    "if Set == 'Protein':\n",
    "    train_dataset = MyDataset(Set, train_PD, None, train_labels, transform=transform_to_use)\n",
    "    val_dataset = MyDataset(Set, val_PD, None, val_labels, transform=None)\n",
    "elif Set == 'mRNA':\n",
    "    train_dataset = MyDataset(Set, None, train_MD, train_labels, transform=transform_to_use)\n",
    "    val_dataset = MyDataset(Set, None, val_MD, val_labels, transform=None)\n",
    "elif Set == 'Protein + mRNA':\n",
    "    train_dataset = MyDataset(Set, train_PD, train_MD, train_labels, transform=transform_to_use)\n",
    "    val_dataset = MyDataset(Set, val_PD, val_MD, val_labels, transform=None)\n",
    "else:\n",
    "    raise ValueError(\"Set must be 'Protein' or 'mRNA', or 'Protein + mRNA'\")\n",
    "\n",
    "# Create the training data loaders, iteration of each index happens here\n",
    "if ImbalanceSampler:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch, shuffle=False, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "    print('Train dataloader is balanced by ImbalancedDatasetSampler')\n",
    "else:\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch, shuffle=True)\n",
    "    print('Train dataloader is not balanced by ImbalancedDatasetSampler')\n",
    "\n",
    "\n",
    "# Create the validation data loader, shuffling is not necessary\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False)\n",
    "print('Validation dataloader is not shuffled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14169f0",
   "metadata": {},
   "source": [
    "### Check quantity of each label in dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba269809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the labels corresponding to each class\n",
    "for i, label in enumerate(encoder.classes_):\n",
    "    print(f\"{i} is {label}\")\n",
    "\n",
    "# Initialize a Counter object\n",
    "train_class_counts = Counter()\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for _, labels in train_dataloader:\n",
    "    # Update the Counter with the labels in the current batch\n",
    "    train_class_counts.update(labels.numpy())\n",
    "\n",
    "# Print the class distribution\n",
    "print(f\"Training set label counts: {sorted(train_class_counts.items())}\")\n",
    "\n",
    "# Initialize a Counter object\n",
    "val_class_counts = Counter()\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for _, labels in val_dataloader:\n",
    "    # Update the Counter with the labels in the current batch\n",
    "    val_class_counts.update(labels.numpy())\n",
    "\n",
    "# Print the class distribution\n",
    "print(f\"Validation set label counts: {sorted(val_class_counts.items())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6729a",
   "metadata": {},
   "source": [
    "### Visualize the tensors in data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train data loader to see the transformed tensors\n",
    "train_batch_images, train_batch_labels = next(iter(train_dataloader))\n",
    "\n",
    "# Use first image in the batch\n",
    "first_image_in_train_batch = train_batch_images[0]\n",
    "image_to_plot = first_image_in_train_batch.permute(1, 2, 0).numpy().astype('uint8')\n",
    "first_label_in_train_batch = train_batch_labels[0]\n",
    "\n",
    "# Visualize the tensor as an image \n",
    "plt.imshow(image_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbe2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train data loader to see the transformed tensors\n",
    "val_batch_images, val_batch_labels = next(iter(val_dataloader))\n",
    "\n",
    "# Use first image in the batch\n",
    "first_image_in_val_batch = val_batch_images[0]\n",
    "image_to_plot = first_image_in_val_batch.permute(1, 2, 0).numpy().astype('uint8')\n",
    "first_label_in_val_batch = val_batch_labels[0]\n",
    "\n",
    "# Visualize the tensor as an image\n",
    "plt.imshow(image_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3278b",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For recording the time and double check the sets before running\n",
    "print(Time_Stamp)\n",
    "print(f'Dataset: {Set}')\n",
    "print(f'Image size: {Canvas_Size} x {Canvas_Size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321926f0",
   "metadata": {},
   "source": [
    "### CustomNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6bc967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "\n",
    "        # Dropout percentage for the model\n",
    "        self.dropout_percentage = 0.3\n",
    "\n",
    "        # Initial layer: 2x2 kernel, stride 1 (input 18x18 -> output 17x17)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv2: 2x2 kernel, stride 1 (output 16x16)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Conv3: 2x2 kernel, stride 1 (output 15x15)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Conv4: 2x2 kernel, stride 2 (output 7x7)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Conv5: 3x3 kernel, stride 1 (output 5x5)\n",
    "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "\n",
    "        # Conv6: 2x2 kernel, stride 1 (output 4x4)\n",
    "        self.conv6 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        self.dropout6 = nn.Dropout(p=self.dropout_percentage)  # Dropout after conv6\n",
    "\n",
    "        # Conv7: 2x2 kernel, stride 2 (output 2x2)\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.dropout7 = nn.Dropout(p=self.dropout_percentage)  # Dropout after conv7\n",
    "\n",
    "        # Adaptive Average Pooling (output 1x1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(512, 1000)\n",
    "        self.fc_dropout = nn.Dropout(p=self.dropout_percentage)  # Dropout after fully connected layer\n",
    "        self.out = nn.Linear(1000, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: 64 channels, 17x17\n",
    "\n",
    "        # Second conv block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: 128 channels, 16x16\n",
    "\n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: 256 channels, 15x15\n",
    "\n",
    "        # Fourth conv block with downsampling\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # Output: 512 channels, 7x7\n",
    "\n",
    "        # Fifth conv block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))  # Output: 512 channels, 5x5\n",
    "\n",
    "        # Sixth conv block with dropout\n",
    "        x = F.relu(self.bn6(self.conv6(x)))  # Output: 512 channels, 4x4\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        # Seventh conv block with dropout\n",
    "        x = F.relu(self.bn7(self.conv7(x)))  # Output: 512 channels, 2x2\n",
    "        x = self.dropout7(x)\n",
    "\n",
    "        # Global Average Pooling to reduce size to 1x1\n",
    "        x = self.avgpool(x)  # Output: 512 channels, 1x1\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc_dropout(x)  # Dropout after fully connected layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08b7d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with initial stride of 2\n",
    "class SimpleResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet2, self).__init__()\n",
    "\n",
    "        # Dropout percentage for the model\n",
    "        self.dropout_percentage = 0.5\n",
    "\n",
    "        # Conv1: 2x2 kernel, stride 2 (input 18x18 -> output 9x9)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Conv2: 2x2 kernel, stride 1 (output 8x8)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Conv3: 2x2 kernel, stride 1 (output 7x7)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(2, 2), stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.dropout3 = nn.Dropout(p=0.3)  # Dropout after conv3\n",
    "\n",
    "        # Conv4: 2x2 kernel, stride 2 (output 3x3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(2, 2), stride=2, padding=0)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.dropout4 = nn.Dropout(p=0.3)  # Dropout after conv4\n",
    "\n",
    "        # Conv5: 3x3 kernel, stride 1 (output 1x1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=(3, 3), stride=1, padding=0)\n",
    "        self.bn5 = nn.BatchNorm2d(1024)\n",
    "        self.dropout5 = nn.Dropout(p=0.4)  # Dropout after conv5\n",
    "\n",
    "        # Conv6: 2x2 kernel, stride 1 (remains 1x1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn6 = nn.BatchNorm2d(512)\n",
    "        self.dropout6 = nn.Dropout(p=self.dropout_percentage)  # Dropout after conv6\n",
    "\n",
    "        # Conv7: 2x2 kernel, stride 2 (remains 1x1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(1, 1), stride=1, padding=0)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.dropout7 = nn.Dropout(p=self.dropout_percentage)  # Dropout after conv7\n",
    "\n",
    "        # Adaptive Average Pooling (output 1x1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Linear(512, 1000)\n",
    "        self.fc_dropout = nn.Dropout(p=self.dropout_percentage)  # Dropout after fully connected layer\n",
    "        self.out = nn.Linear(1000, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # Output: 64 channels, 9x9\n",
    "\n",
    "        # Second conv block\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # Output: 128 channels, 8x8\n",
    "\n",
    "        # Third conv block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # Output: 256 channels, 7x7\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Fourth conv block with downsampling\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # Output: 512 channels, 3x3\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        # Fifth conv block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))  # Output: 1024 channels, 1x1\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        # Sixth conv block with dropout\n",
    "        x = F.relu(self.bn6(self.conv6(x)))  # Output: 512 channels, 1x1\n",
    "        x = self.dropout6(x)\n",
    "\n",
    "        # Seventh conv block with dropout\n",
    "        x = F.relu(self.bn7(self.conv7(x)))  # Output: 512 channels, 1x1\n",
    "        x = self.dropout7(x)\n",
    "\n",
    "        # Global Average Pooling to reduce size to 1x1\n",
    "        x = self.avgpool(x)  # Output: 512 channels, 1x1\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc_dropout(x)  # Dropout after fully connected layer\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf6a763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelPreservingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelPreservingNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers without padding\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=2)  # 18x18 -> 17x17\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2) # 17x17 -> 16x16\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2) # 16x16 -> 15x15\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(64 * 15 * 15, 4)  # Flattened size is 256*10*10\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # 18x18 -> 16x16\n",
    "        \n",
    "        # Block 2\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # 16x16 -> 14x14\n",
    "        \n",
    "        # Block 3\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # 14x14 -> 12x12\n",
    "        \n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109d1f6",
   "metadata": {},
   "source": [
    "### Training and testing epoch loop functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "663d6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training the model\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Prepare data\n",
    "        X = X.to(torch.float32) # convert to float32 to avoid error stating byte expected but found float\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass: predict classes\n",
    "        pred = model(X)\n",
    "    \n",
    "        # Compute the cross-entropy loss \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = F.softmax(pred, dim=1)\n",
    "\n",
    "        # Compute the entropy of the softmax output for the confidence penalty\n",
    "        entropy = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-10), dim=1)\n",
    "\n",
    "        # Confidence penalty: encourage high entropy (i.e., reduce overconfidence)\n",
    "        penalty = torch.mean(entropy)\n",
    "        \n",
    "        # Total loss: cross-entropy loss + confidence penalty (scaled by lambda); lambda set to 0.1\n",
    "        total_loss_with_penalty = loss - 0.1 * penalty\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # zero the parameter gradients \n",
    "        total_loss_with_penalty.backward() # backpropagate the loss\n",
    "\n",
    "        # Compute gradient norms for each parameter\n",
    "        batch_gradient_norms = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.data.norm(2).item()  # Compute L2 norm\n",
    "                batch_gradient_norms.append((name, grad_norm))\n",
    "        \n",
    "        # Log or print the gradient norms for this batch\n",
    "        gradient_norms.append(batch_gradient_norms)  # Store gradient norms to analyze later\n",
    "        \n",
    "        # Step the optimizer\n",
    "        optimizer.step() # adjust parameters based on the calculated gradients\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item() # extract the loss value\n",
    "\n",
    "        # Get the current learing rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    if LrScheduler:\n",
    "        exp_lr_scheduler.step()   \n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return avg_loss, current_lr, gradient_norms\n",
    "            \n",
    "# Function for validating the model\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, accuracy = 0, 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        \n",
    "        # Prepare data\n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass: predict classes\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Compute the softmax probabilities\n",
    "        softmax_probs = F.softmax(pred, dim=1)\n",
    "\n",
    "        # # Compute the loss\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        # Calculate accuracy: choose the class with the highest probability\n",
    "        accuracy += (softmax_probs.argmax(1) == y).type(torch.float).sum().item()\n",
    "        #accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy /= size\n",
    "        \n",
    "    return test_loss, accuracy\n",
    "\n",
    "# Function for testing the model with confusion matrix, and softmax output with a condition of >50% certanty\n",
    "def test_loop_with_confusion_matrix(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, accuracy, no_prediction_count = 0, 0, 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        # convert to float32 to avoid byte/float mismatch error\n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(pred, dim=1)\n",
    "        \n",
    "        # Get the maximum probability and its corresponding class\n",
    "        max_prob, predicted_class = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        # Only consider predictions with a probability greater than 0.5\n",
    "        valid_predictions = max_prob > 0.5\n",
    "        \n",
    "        # Convert the labels to the same device and calculate the loss\n",
    "        y = y.to(device)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        \n",
    "        # Append valid predictions and corresponding labels to lists for confusion matrix\n",
    "        all_preds.extend(predicted_class[valid_predictions].cpu().numpy())\n",
    "        all_labels.extend(y[valid_predictions].cpu().numpy())\n",
    "        \n",
    "        # Count valid predictions where probability is greater than 0.5\n",
    "        valid_preds_count = valid_predictions.sum().item()\n",
    "        \n",
    "        # Calculate accuracy only for valid predictions\n",
    "        accuracy += (predicted_class[valid_predictions] == y[valid_predictions]).type(torch.float).sum().item()\n",
    "        \n",
    "        # Count how many predictions were skipped (i.e., where no probability exceeded 0.5)\n",
    "        no_prediction_count += (valid_predictions == 0).sum().item()\n",
    "\n",
    "        # Calculate the precision and recall\n",
    "        precision = precision_score(all_labels, all_preds, average=None, zero_division=0.0)\n",
    "        recall = recall_score(all_labels, all_preds, average=None, zero_division=0.0)\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    if size == no_prediction_count:\n",
    "        accuracy = 0\n",
    "    else:\n",
    "        accuracy /= (size - no_prediction_count)  # Normalize accuracy by valid predictions\n",
    "    no_prediction_percentage = no_prediction_count / size * 100\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Skipped predictions (no class > 50%): {no_prediction_percentage:.2f}%\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    return test_loss, accuracy, conf_matrix, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f0566c",
   "metadata": {},
   "source": [
    "### Confusion matrix, precision and recall calculation helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c26a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the precision and recall\n",
    "def calculate_precision_recall(dataloader, model):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # iterate over validation data\n",
    "    for X, y in dataloader:\n",
    "        \n",
    "        X = X.to(torch.float32)\n",
    "        X = X.to(device)\n",
    "        output = model(X)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # Save Prediction\n",
    "\n",
    "        labels = y.cpu().numpy()\n",
    "        y_true.extend(labels)  # Save Truth\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate precision and recall for each class\n",
    "    precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "    return cf_matrix, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d1232",
   "metadata": {},
   "source": [
    "### F1 score helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59556bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_metrics(conf_matrix):\n",
    "    # True positives are the diagonal elements\n",
    "    tp = np.diag(conf_matrix)\n",
    "    # False positives are the sum of each column minus the diagonal element\n",
    "    fp = np.sum(conf_matrix, axis=0) - tp\n",
    "    # False negatives are the sum of each row minus the diagonal element\n",
    "    fn = np.sum(conf_matrix, axis=1) - tp\n",
    "    # True negatives are the sum of all elements minus the sum of the corresponding row and column plus tp\n",
    "    tn = conf_matrix.sum() - (fp + fn + tp)\n",
    "    \n",
    "    # Precision, recall, and F1 score calculations\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Return macro-averaged F1 score (excluding NaNs caused by divisions by zero)\n",
    "    return np.nanmean(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e94c70",
   "metadata": {},
   "source": [
    "### Define early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b50c98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0001, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved.\n",
    "            min_delta (float): Minimum change in validation loss to qualify as an improvement.\n",
    "            verbose (bool): If True, prints a message for each improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset the counter if there's an improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bdc13",
   "metadata": {},
   "source": [
    "### Define loss functions and class weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9d88c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        \"\"\"\n",
    "        :param gamma: focusing parameter (default=2.0)\n",
    "        :param alpha: balance parameter, it can be a float or a tensor (default=None)\n",
    "        :param reduction: specify the reduction to apply to the output: 'none' | 'mean' | 'sum' (default='mean')\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute cross-entropy loss\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # pt is the probability of the true class\n",
    "        \n",
    "        # Compute the focal loss\n",
    "        F_loss = (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        # Apply class weighting (alpha) if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha[targets]\n",
    "            F_loss = alpha * F_loss\n",
    "        \n",
    "        # Apply the specified reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return F_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return F_loss.sum()\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5baf7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# https://medium.com/@zergtant/use-weighted-loss-function-to-solve-imbalanced-data-classification-problems-749237f38b75\n",
    "if ApplyClassWeightToLoss:\n",
    "    # Weights for each class for loss function\n",
    "    LossWeights = torch.tensor([0.8,2.7,1.3,1.6]) # calculate by log inverse class frequency np.log(total/(# + 1)), it's better\n",
    "    # LossWeights = torch.tensor([0.569, 3.571, 0.864, 1.241]) # inverse class frequency weight wj=N/(nj*4)\n",
    "    # LossWeights = torch.tensor([0.569, 3.559, 0.864, 1.239]) # inverse class frequency weight smoothed wj=(N+smooth)/(nj+smooth*4)\n",
    "    # LossWeights = torch.tensor([1.000, 2.836, 1.417, 1.779]) # log scaling wj=1+(log(max(majority class))/nj)\n",
    "    # LossWeights = torch.tensor([1.139, 7.142, 1.728, 2.481]) # balanced weight wj = N/(nj*2)\n",
    "    \n",
    "    LossWeights = LossWeights.to(device)\n",
    "    \n",
    "    if LossFunc_Name == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing, weight=LossWeights)\n",
    "\n",
    "    elif LossFunc_Name == 'Focal':\n",
    "        loss_fn = FocalLoss(gamma=2, alpha=torch.exp(LossWeights)/torch.exp(LossWeights).sum(), reduction='mean') # Result: tensor([0.1213, 0.5296, 0.1916, 0.1576])\n",
    "\n",
    "else:\n",
    "    if LossFunc_Name == 'CrossEntropy':\n",
    "        loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing)\n",
    "    \n",
    "    elif LossFunc_Name == 'Focal':\n",
    "        loss_fn = FocalLoss(gamma=2, alpha=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95e79a8",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "if Model == 'ResNet18': # Resnet18\n",
    "    model = models.resnet18(weights=None) # without initial weights defined - good for patients not re-arranged\n",
    "    model.fc = nn.Sequential(\n",
    "        model.fc,\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1000, 4))\n",
    "\n",
    "elif Model == 'CustomNN-reg':\n",
    "    model = SimpleResNet() # for 18x18\n",
    "\n",
    "elif Model == 'CustomNN-stride2':\n",
    "    model = SimpleResNet2() # for 18x18 with initial stride 2\n",
    "\n",
    "elif Model == 'PixelPreserve':\n",
    "    model = PixelPreservingNet() # for 12x12\n",
    "\n",
    "model = model.to(dev)\n",
    "\n",
    "# see the model architecture if desired\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8924fc2",
   "metadata": {},
   "source": [
    "### Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e45e0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "def Optimizer(name, model):\n",
    "    if name == 'SGD_weight':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=Learner_rate, weight_decay=Weight_Decay, momentum=Momentum)\n",
    "    if name == 'SGD_default':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=Learner_rate)\n",
    "    if name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Learner_rate, weight_decay=Weight_Decay)\n",
    "    if name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=Learner_rate, betas=(0.9, 0.999), weight_decay=Weight_Decay)\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "# call optimizer\n",
    "optimizer = Optimizer(Optim_Name, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378358a",
   "metadata": {},
   "source": [
    "### Learner rate finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "222f7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_lr_finder import LRFinder\n",
    "# amp_config = {\n",
    "#     'device_type': 'cuda',\n",
    "#     'dtype': torch.float32,\n",
    "# }\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=Learner_rate, betas=(0.9,0.999), weight_decay=Weight_Decay)\n",
    "# grad_scaler = torch.cuda.amp.GradScaler()\n",
    "# lr_finder = LRFinder(model, optimizer, loss_fn, device=\"cuda\", amp_backend='torch', amp_config=amp_config, grad_scaler=grad_scaler)\n",
    "# lr_finder.range_test(train_dataloader, end_lr=1, num_iter=5000, step_mode='exp')\n",
    "# lr_finder.plot()\n",
    "# lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf1cd7",
   "metadata": {},
   "source": [
    "### Define the learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "792db5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LR scheduler\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# Define the cosine annealing scheduler\n",
    "# exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=1e-5)\n",
    "\n",
    "# Define the cosine annealing warm restarts scheduler\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2, eta_min=0.01*Learner_rate) # 1% of initial learning rate as minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f4119-2a9d-4c51-9242-ecc2e14d72bb",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6957",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(Seed)\n",
    "\n",
    "# Set up performance dataframe to record loss and accuracy of each epoch\n",
    "PerformanceDF = pd.DataFrame(index=range(epochs),columns=['Epoch','TrainLoss','ValLoss','TrainAcc','ValAcc'])\n",
    "print(f\"{'Epoch':>5s}{'Train Loss':>13s}{'Val Loss':>11s}{'Train Accuracy':>17s}{'Val Accuracy':>15s}{'Current LR':>13s}\\n\")\n",
    "PrecisionRecallDF = pd.DataFrame(columns=['Epoch', 'Class', 'Precision', 'Recall'])\n",
    "index = 0\n",
    "current_lr = Learner_rate\n",
    "\n",
    "# Initialize early stopping\n",
    "if Earlystopping == True:\n",
    "    early_stopping = EarlyStopping(patience=30, min_delta=0.0001, verbose=True)\n",
    "\n",
    "# epochs\n",
    "for t in range(epochs+1):\n",
    "\n",
    "    if t == 0:\n",
    "        # get initial model losses, accuracies from both training and validation sets\n",
    "        train_loss, train_accuracy = test_loop(train_dataloader, model, loss_fn)\n",
    "        val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n",
    "        \n",
    "        # get initial model precision and recall for each class\n",
    "        val_cf_matrix, precision, recall = calculate_precision_recall(val_dataloader, model)\n",
    "\n",
    "    else:\n",
    "        # training the model, output the losses, get the training and validation accuracy on the fly\n",
    "        model.train()\n",
    "        train_loss, current_lr, gradient_norms = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        _, train_accuracy = test_loop(train_dataloader, model, loss_fn)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_accuracy = test_loop(val_dataloader, model, loss_fn)\n",
    "            val_cf_matrix, precision, recall = calculate_precision_recall(val_dataloader, model)\n",
    "        \n",
    "    Epoch = t\n",
    "    print(f\"{str(Epoch):>5s}{train_loss:>13f}{val_loss:>11f}{train_accuracy:>16f}{val_accuracy:>14f}{current_lr:>16f}\")\n",
    "    PerformanceDF.loc[t,:]=[Epoch,train_loss,val_loss,train_accuracy,val_accuracy]\n",
    "    for i in range(4):\n",
    "        PrecisionRecallDF.loc[index]=[Epoch, i, precision[i], recall[i]]\n",
    "        index += 1\n",
    "    \n",
    "    if t == 0:\n",
    "        highest_val_accuracy = 0 \n",
    "        MSD0 = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    if val_accuracy > highest_val_accuracy:\n",
    "        highest_val_accuracy = val_accuracy\n",
    "        MSD_Best = copy.deepcopy(model.state_dict())\n",
    "        # Record the epoch when the best model is saved\n",
    "        Best_Epoch = t\n",
    "        torch.save({\n",
    "            'model_state_dict': MSD_Best,\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \n",
    "            Path(model_folder_path + f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "\n",
    "    if t == epochs:\n",
    "        MSD_Final = copy.deepcopy(model.state_dict())\n",
    "        torch.save({\n",
    "            'model_state_dict': MSD_Final,\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \n",
    "            Path(model_folder_path + f'last_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if Earlystopping == True:\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break  # Stop the training loop if early stopping is triggered\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6279727",
   "metadata": {},
   "source": [
    "## Testing set prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca84d2",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27652e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the synthetic data\n",
    "if Set == 'Protein + mRNA':\n",
    "    # Johansson\n",
    "    J_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_dual_PD_synthetic_kde.csv'))\n",
    "    J_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_dual_MD_synthetic_kde.csv'))\n",
    "    \n",
    "    # Wrangling\n",
    "    J_PD_synthetic.index = J_PD_synthetic.loc[:,'Index']\n",
    "    J_MD_synthetic.index = J_MD_synthetic.loc[:,'Unnamed: 0']\n",
    "    J_PD_synthetic = J_PD_synthetic.loc[:, J_PD_synthetic.columns!='Index']\n",
    "    J_MD_synthetic = J_MD_synthetic.loc[:, J_MD_synthetic.columns!='Unnamed: 0']\n",
    "\n",
    "\n",
    "    # Mertins\n",
    "    M_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_dual_PD_synthetic_kde.csv'))\n",
    "    M_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_dual_MD_synthetic_kde.csv'))\n",
    "    \n",
    "    # Wrangling\n",
    "    M_PD_synthetic.index = M_PD_synthetic.loc[:,'Index']\n",
    "    M_MD_synthetic.index = M_MD_synthetic.loc[:,'gene_name']\n",
    "    M_PD_synthetic = M_PD_synthetic.loc[:, M_PD_synthetic.columns!='Index']\n",
    "    M_MD_synthetic = M_MD_synthetic.loc[:, M_MD_synthetic.columns!='gene_name']\n",
    "\n",
    "    # Krug\n",
    "    K_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'K_dual_PD_synthetic_kde.csv'))\n",
    "    K_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'K_dual_MD_synthetic_kde.csv'))\n",
    "\n",
    "    # Wrangling\n",
    "    K_PD_synthetic.index = K_PD_synthetic.loc[:,'Index']\n",
    "    K_MD_synthetic.index = K_MD_synthetic.loc[:,'gene_name']\n",
    "    K_PD_synthetic = K_PD_synthetic.loc[:, K_PD_synthetic.columns!='Index']\n",
    "    K_MD_synthetic = K_MD_synthetic.loc[:, K_MD_synthetic.columns!='gene_name']\n",
    "    \n",
    "    print(f\"Set is 'Protein + mRNA'\")\n",
    "\n",
    "elif Set == 'Protein':\n",
    "    # Johansson\n",
    "    J_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_single_PD_synthetic.csv'))\n",
    "    J_PD_synthetic.index = J_PD_synthetic.loc[:,'Index']\n",
    "    J_PD_synthetic = J_PD_synthetic.loc[:, J_PD_synthetic.columns!='Index']\n",
    "\n",
    "    # Mertins\n",
    "    M_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_single_PD_synthetic.csv'))\n",
    "    M_PD_synthetic.index = M_PD_synthetic.loc[:,'gene_name']\n",
    "    M_PD_synthetic = M_PD_synthetic.loc[:, M_PD_synthetic.columns!='gene_name']\n",
    "\n",
    "    # Krug\n",
    "    K_PD_synthetic = pd.read_csv(Path(dataset_folder_path+'K_single_PD_synthetic.csv'))\n",
    "    K_PD_synthetic.index = K_PD_synthetic.loc[:,'Gene']\n",
    "    K_PD_synthetic = K_PD_synthetic.loc[:, K_PD_synthetic.columns!='Gene']\n",
    "    print(f\"Set is 'Protein'\")\n",
    "\n",
    "elif Set == 'mRNA':\n",
    "    # Johansson\n",
    "    J_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'J_single_MD_synthetic.csv'))\n",
    "    J_MD_synthetic.index = J_MD_synthetic.loc[:,'Unnamed: 0']\n",
    "    J_MD_synthetic = J_MD_synthetic.loc[:, J_MD_synthetic.columns!='Unnamed: 0']\n",
    "\n",
    "    # Mertins\n",
    "    M_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'M_single_MD_synthetic_kde.csv'))\n",
    "    M_MD_synthetic.index = M_MD_synthetic.loc[:,'gene_name']\n",
    "    M_MD_synthetic = M_MD_synthetic.loc[:, M_MD_synthetic.columns!='gene_name']\n",
    "\n",
    "    # Krug\n",
    "    K_MD_synthetic = pd.read_csv(Path(dataset_folder_path+'K_single_MD_synthetic.csv'))\n",
    "    K_MD_synthetic.index = K_MD_synthetic.loc[:,'gene_name']\n",
    "    K_MD_synthetic = K_MD_synthetic.loc[:, K_MD_synthetic.columns!='gene_name']\n",
    "    print(f\"Set is 'mRNA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9cff554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the label\n",
    "LD = pd.read_csv(Path(dataset_folder_path+'SubCellBarcode.MCF7.txt'), sep='\\t')\n",
    "# Data set wrangling\n",
    "LD.index = LD.loc[:,'Protein']\n",
    "LD = LD.loc[:,LD.columns!='Protein']\n",
    "# Remove unclassified in K_LD\n",
    "NotUnclassInd = LD.loc[:,'Localization'] != 'Unclassified'\n",
    "LD = LD.loc[NotUnclassInd,:]\n",
    "\n",
    "if Set == 'Protein + mRNA':\n",
    "    J_LD = LD.loc[J_PD_synthetic.index,:]\n",
    "    M_LD = LD.loc[M_PD_synthetic.index,:]\n",
    "    K_LD = LD.loc[K_PD_synthetic.index,:]\n",
    "\n",
    "elif Set == 'Protein':\n",
    "    J_LD = LD.loc[J_PD_synthetic.index,:]\n",
    "    M_LD = LD.loc[M_PD_synthetic.index,:]\n",
    "    K_LD = LD.loc[K_PD_synthetic.index,:]\n",
    "\n",
    "elif Set == 'mRNA':\n",
    "    J_LD = LD.loc[J_MD_synthetic.index,:]\n",
    "    M_LD = LD.loc[M_MD_synthetic.index,:]\n",
    "    K_LD = LD.loc[K_MD_synthetic.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count labels and report the count and percentage\n",
    "J_LD_count = J_LD.value_counts()\n",
    "M_LD_count = M_LD.value_counts()\n",
    "K_LD_count = K_LD.value_counts()\n",
    "\n",
    "# Print the label count and percentage\n",
    "print(f'Johansson label count:\\n{J_LD_count}\\n')\n",
    "print(f'Mertins label count:\\n{M_LD_count}\\n')\n",
    "print(f'Krug label count:\\n{K_LD_count}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12be44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "set_seed(Seed)\n",
    "\n",
    "# Create a LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the labels to integers from LD, the label df\n",
    "J_labels = encoder.fit_transform(J_LD.values.ravel())\n",
    "M_labels = encoder.fit_transform(M_LD.values.ravel())\n",
    "K_labels = encoder.fit_transform(K_LD.values.ravel())\n",
    "\n",
    "# Convert the labels to tensors\n",
    "J_labels = torch.tensor(J_labels)\n",
    "M_labels = torch.tensor(M_labels)\n",
    "K_labels = torch.tensor(K_labels)\n",
    "\n",
    "# Check Dataset argument to determine which dataset to use\n",
    "if Set == 'Protein':\n",
    "    J_test_dataset = MyDataset(Set, J_PD_synthetic, None, J_labels, transform=None)\n",
    "    M_test_dataset = MyDataset(Set, M_PD_synthetic, None, M_labels, transform=None)\n",
    "    K_test_dataset = MyDataset(Set, K_PD_synthetic, None, K_labels, transform=None)\n",
    "    print('Protein dataset loaded')\n",
    "elif Set == 'mRNA':\n",
    "    J_test_dataset = MyDataset(Set, None, J_MD_synthetic, J_labels, transform=None)\n",
    "    M_test_dataset = MyDataset(Set, None, M_MD_synthetic, M_labels, transform=None)\n",
    "    K_test_dataset = MyDataset(Set, None, K_MD_synthetic, K_labels, transform=None)\n",
    "    print('mRNA dataset loaded')\n",
    "elif Set == 'Protein + mRNA':\n",
    "    J_test_dataset = MyDataset(Set, J_PD_synthetic, J_MD_synthetic, J_labels, transform=None)\n",
    "    M_test_dataset = MyDataset(Set, M_PD_synthetic, M_MD_synthetic, M_labels, transform=None)\n",
    "    K_test_dataset = MyDataset(Set, K_PD_synthetic, K_MD_synthetic, K_labels, transform=None)\n",
    "    print('Protein + mRNA dataset loaded')\n",
    "else:\n",
    "    raise ValueError(\"Set must be 'Protein', 'mRNA' or 'Protein + mRNA'\")\n",
    "\n",
    "# Create the dataloaders\n",
    "J_test_dataloader = DataLoader(J_test_dataset, batch_size=batch, shuffle=False)\n",
    "M_test_dataloader = DataLoader(M_test_dataset, batch_size=batch, shuffle=False)\n",
    "K_test_dataloader = DataLoader(K_test_dataset, batch_size=batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the train data loader to see the transformed tensors\n",
    "J_test_batch_images, J_test_batch_labels = next(iter(J_test_dataloader))\n",
    "\n",
    "# Use first image in the batch\n",
    "first_image_in_J_test_batch = J_test_batch_images[0]\n",
    "image_to_plot = first_image_in_J_test_batch.permute(1, 2, 0).numpy().astype('uint8')\n",
    "\n",
    "# Visualize the tensor as an image\n",
    "plt.imshow(image_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27510fc4",
   "metadata": {},
   "source": [
    "### Re-define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fe68a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LossFunc_Name == 'CrossEntropy':\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=Label_Smoothing)\n",
    "\n",
    "elif LossFunc_Name == 'Focal':\n",
    "    loss_fn = FocalLoss(gamma=2, alpha=None, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae924a30",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384f744-1e3c-4c19-8b53-89d88024a4b8",
   "metadata": {},
   "source": [
    "### Fig 1: training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best epoch at: {Best_Epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573797e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph loss for training and validation\n",
    "figure1 = plt.gcf()\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['TrainLoss'])\n",
    "plt.plot(PerformanceDF['Epoch'], PerformanceDF['ValLoss'])\n",
    "plt.xlim([0,Best_Epoch])\n",
    "if LossFunc_Name == 'CrossEntropy':\n",
    "    plt.ylim([0, 1.8])\n",
    "elif LossFunc_Name == 'Focal':\n",
    "    plt.ylim([0, 0.3])\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tick_params(axis='x', labelsize=13)\n",
    "plt.tick_params(axis='y', labelsize=13)\n",
    "plt.legend(['Training', 'Validation'], fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fa803b",
   "metadata": {},
   "source": [
    "### Fig 1.5: gradient norms across training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb68988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average gradient norm for each batch\n",
    "average_gradient_norms = [\n",
    "    sum(norm for _, norm in batch) / len(batch)\n",
    "    for batch in gradient_norms\n",
    "]\n",
    "\n",
    "# Plot the average gradient norm per batch\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(average_gradient_norms) + 1), average_gradient_norms, label=\"Average Gradient Norm per Batch\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Average Gradient Norm\")\n",
    "plt.title(\"Average Gradient Norm Across Batches\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1232ff6",
   "metadata": {},
   "source": [
    "### Fig 2: training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph accuracy for training and validation\n",
    "figure2 = plt.gcf()\n",
    "plt.plot(PerformanceDF['Epoch'], (PerformanceDF['TrainAcc'])*100)\n",
    "plt.plot(PerformanceDF['Epoch'], (PerformanceDF['ValAcc'])*100)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tick_params(axis='x', labelsize=13)\n",
    "plt.tick_params(axis='y', labelsize=13)\n",
    "plt.xlim([0,Best_Epoch])\n",
    "plt.ylim([0,100])\n",
    "plt.legend(['Training', 'Validation'], fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph error rate for training and validation\n",
    "figure2_5 = plt.gcf()\n",
    "plt.plot(PerformanceDF['Epoch'], (1-PerformanceDF['TrainAcc'])*100)\n",
    "plt.plot(PerformanceDF['Epoch'], (1-PerformanceDF['ValAcc'])*100)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Error Rate (%)', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tick_params(axis='x', labelsize=13)\n",
    "plt.tick_params(axis='y', labelsize=13)\n",
    "plt.xlim([0,Best_Epoch])\n",
    "plt.ylim([0,100])\n",
    "plt.legend(['Training', 'Validation'], fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ca104",
   "metadata": {},
   "source": [
    "### Fig 3: validation confusion matrix by best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4beabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the best model\n",
    "if Model == 'CustomNN-reg':\n",
    "    best_model = SimpleResNet()\n",
    "    checkpoint = torch.load(Path(model_folder_path+f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "elif Model == 'CustomNN-stride2':\n",
    "    best_model = SimpleResNet2()\n",
    "    checkpoint = torch.load(Path(model_folder_path+f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "elif Model == 'ResNet18':\n",
    "    best_model = models.resnet18(weights=None)\n",
    "    best_model.fc = nn.Sequential(\n",
    "        best_model.fc,\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1000, 4))\n",
    "    checkpoint = torch.load(Path(model_folder_path+f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "elif Model == 'PixelPreserve':\n",
    "    best_model = PixelPreservingNet()\n",
    "    checkpoint = torch.load(Path(model_folder_path+f'best_model_{Run_Name}_{Time_Stamp}.pth'))\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "best_model.to(device)\n",
    "\n",
    "# Evalulate validation set (accuracy, F1, confusion matrix)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Accuracy\n",
    "    val_loss, val_accuracy = test_loop(val_dataloader, best_model, loss_fn)\n",
    "    print(f\"Validation accuracy by best model: {round(val_accuracy, 3)}\")\n",
    "    \n",
    "    # Confusion matrix, precision, recall\n",
    "    val_cf_matrix, val_precision, val_recall = calculate_precision_recall(val_dataloader, best_model)\n",
    "    print(f'Validation precision by best model: Cytosol {round(val_precision[0],3)}, Mitochondrial {round(val_precision[1],3)}, Nuclear {round(val_precision[2],3)}, Secretory {round(val_precision[3],3)}')\n",
    "    print(f'Validation recall by best model: Cytosol {round(val_recall[0],3)}, Mitochondrial {round(val_recall[1],3)}, Nuclear {round(val_recall[2],3)}, Secretory {round(val_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    val_f1 = calculate_f1_metrics(val_cf_matrix)\n",
    "    print(f'Validation F1 score by best model is {round(val_f1, 3)}')\n",
    "\n",
    "# Confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=val_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "figure3 = plt.savefig(Path(figure_folder_path+f'figure3_val_cm_best_model_{Run_Name}_{Time_Stamp}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9132813",
   "metadata": {},
   "source": [
    "### Fig 4: testing confusion matrics by best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slient warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Evalulate test set (accuracy, precision, recall, F1)\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    # Johansson test set\n",
    "    # Accuracy, precision, recall\n",
    "    #J_test_loss, J_test_accuracy, J_test_cf_matrix, J_precision, J_recall = test_loop_with_confusion_matrix(J_test_dataloader, best_model, loss_fn)\n",
    "    J_test_loss, J_test_accuracy = test_loop(J_test_dataloader, best_model, loss_fn)\n",
    "    J_test_cf_matrix, J_precision, J_recall = calculate_precision_recall(J_test_dataloader, best_model)\n",
    "    print(f\"Johansson testing accuracy by best model: {round(J_test_accuracy, 3)}\")\n",
    "    if J_test_accuracy == 0:\n",
    "        pass\n",
    "    else:\n",
    "        print(f'Johansson testing precision by best model: Cytosol {round(J_precision[0],3)}, Mitochondrial {round(J_precision[1],3)}, Nuclear {round(J_precision[2],3)}, Secretory {round(J_precision[3],3)}')\n",
    "        print(f'Johansson testing recall by best model: Cytosol {round(J_recall[0],3)}, Mitochondrial {round(J_recall[1],3)}, Nuclear {round(J_recall[2],3)}, Secretory {round(J_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    J_test_f1 = calculate_f1_metrics(J_test_cf_matrix)\n",
    "    print(f'Johansson testing F1 score by best model is {round(J_test_f1, 3)}')\n",
    "\n",
    "    # Mertins test set\n",
    "    # Accuracy, precision, recall\n",
    "    #M_test_loss, M_test_accuracy, M_test_cf_matrix, M_precision, M_recall = test_loop_with_confusion_matrix(M_test_dataloader, best_model, loss_fn)\n",
    "    M_test_loss, M_test_accuracy = test_loop(M_test_dataloader, best_model, loss_fn)\n",
    "    M_test_cf_matrix, M_precision, M_recall = calculate_precision_recall(M_test_dataloader, best_model)\n",
    "    print(f\"Mertins testing accuracy by best model: {round(M_test_accuracy, 3)}\")\n",
    "    if M_test_accuracy == 0:\n",
    "        pass\n",
    "    else:\n",
    "        print(f'Mertins testing precision by best model: Cytosol {round(M_precision[0],3)}, Mitochondrial {round(M_precision[1],3)}, Nuclear {round(M_precision[2],3)}, Secretory {round(M_precision[3],3)}')\n",
    "        print(f'Mertins testing recall by best model: Cytosol {round(M_recall[0],3)}, Mitochondrial {round(M_recall[1],3)}, Nuclear {round(M_recall[2],3)}, Secretory {round(M_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    M_test_f1 = calculate_f1_metrics(M_test_cf_matrix)\n",
    "    print(f'Mertins testing F1 score by best model is {round(M_test_f1, 3)}')\n",
    "\n",
    "    # Krug test set\n",
    "    # Accuracy, precision, recall\n",
    "    #K_test_loss, K_test_accuracy, K_test_cf_matrix, K_precision, K_recall = test_loop_with_confusion_matrix(K_test_dataloader, best_model, loss_fn)\n",
    "    K_test_loss, K_test_accuracy = test_loop(K_test_dataloader, best_model, loss_fn)\n",
    "    K_test_cf_matrix, K_precision, K_recall = calculate_precision_recall(K_test_dataloader, best_model)\n",
    "    print(f\"Krug testing accuracy by best model: {round(K_test_accuracy, 3)}\")\n",
    "    if K_test_accuracy == 0:\n",
    "        pass\n",
    "    else:\n",
    "        print(f'Krug testing precision by best model: Cytosol {round(K_precision[0],3)}, Mitochondrial {round(K_precision[1],3)}, Nuclear {round(K_precision[2],3)}, Secretory {round(K_precision[3],3)}')\n",
    "        print(f'Krug testing recall by best model: Cytosol {round(K_recall[0],3)}, Mitochondrial {round(K_recall[1],3)}, Nuclear {round(K_recall[2],3)}, Secretory {round(K_recall[3],3)}')\n",
    "\n",
    "    # F1\n",
    "    K_test_f1 = calculate_f1_metrics(K_test_cf_matrix)\n",
    "    print(f'Krug testing F1 score by best model is {round(K_test_f1, 3)}')\n",
    "\n",
    "# Display the confusion matrices side by side\n",
    "if J_test_accuracy == 0:\n",
    "    pass\n",
    "else:\n",
    "    disp_J = ConfusionMatrixDisplay(confusion_matrix=J_test_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "    disp_J.plot(cmap=plt.cm.Blues)\n",
    "if M_test_accuracy == 0:\n",
    "    pass\n",
    "else:\n",
    "    disp_M = ConfusionMatrixDisplay(confusion_matrix=M_test_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "    disp_M.plot(cmap=plt.cm.Blues)\n",
    "if K_test_accuracy == 0:\n",
    "    pass\n",
    "else:\n",
    "    disp_K = ConfusionMatrixDisplay(confusion_matrix=K_test_cf_matrix, display_labels=['Cytosolic', 'Mitochondrial', 'Nuclear', 'Secretory'])\n",
    "    disp_K.plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576742b",
   "metadata": {},
   "source": [
    "### Fig 5: Precision and recall of validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cdcc18",
   "metadata": {},
   "source": [
    "#### 95% CI simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbde0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1000 seeds to get the initial precision and recall for thousand times\n",
    "num_of_seeds = list(range(100, 100+1000))\n",
    "index = 0\n",
    "InitialPrecisionRecallDF = pd.DataFrame(columns=['Seed', 'Class', 'Precision', 'Recall'])\n",
    "\n",
    "# We will shuffle the validation dataloader\n",
    "shuffle_val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=True)\n",
    "\n",
    "# Loop through the seeds to get the initial precision and recall\n",
    "for t in num_of_seeds:\n",
    "    set_seed(t)\n",
    "    \n",
    "    # Determine the model\n",
    "    if Model == 'CustomNN-reg':\n",
    "        model_for_ci = SimpleResNet()\n",
    "    elif Model == 'CustomNN-stride2':\n",
    "        model_for_ci = SimpleResNet2()\n",
    "    elif Model == 'CustomNN-mini':\n",
    "        model_for_ci = SimpleResNet_Mini()\n",
    "    elif Model == 'PixelPreserve':\n",
    "        model_for_ci = PixelPreservingNet()\n",
    "    else:\n",
    "        raise ValueError(\"Model must be 'CustomNN-reg', 'CustomNN-stride2', or 'CustomNN-mini'\")\n",
    "\n",
    "    model_for_ci.to(device)\n",
    "\n",
    "    # Get initial model precision and recall for each class\n",
    "    cf, precision, recall = calculate_precision_recall(shuffle_val_dataloader, model_for_ci)\n",
    "    \n",
    "    # Record the initial precision and recall\n",
    "    for i in range(4):\n",
    "        InitialPrecisionRecallDF.loc[index]=[t, i, precision[i], recall[i]]\n",
    "        index += 1\n",
    "\n",
    "    # Print out t to keep track of the progress, if t is divisible by 100\n",
    "    if t % 100 == 0:\n",
    "        print(t)\n",
    "    \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ed085",
   "metadata": {},
   "source": [
    "#### Simulation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Silent the warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Assign class names\n",
    "class_names = ['Cytosolic', 'Mitochondria', 'Nuclear', 'Secretory']\n",
    "\n",
    "# Visualize the precision distribution for each class in a histogram\n",
    "fig, axes = plt.subplots(2,1,figsize=(8, 8))\n",
    "\n",
    "# Define colormaps for each class\n",
    "colormaps = [cm.Blues, cm.Oranges, cm.Greens, cm.Purples]\n",
    "\n",
    "# Visualize the precision distribution for each class in a histogram\n",
    "for i, class_name in enumerate(class_names):\n",
    "    sns.histplot(data=InitialPrecisionRecallDF[InitialPrecisionRecallDF['Class'] == i], \n",
    "                 x='Precision', kde=True, bins=25, element='step', ax=axes[0], color=colormaps[i](0.6), label=class_name)\n",
    "axes[0].set_title(f'Initial Precision Distribution for each class with {Model}')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].legend()\n",
    "\n",
    "# Visualize the recall distribution for each class in a histogram\n",
    "for i, class_name in enumerate(class_names):\n",
    "    sns.histplot(data=InitialPrecisionRecallDF[InitialPrecisionRecallDF['Class'] == i], \n",
    "                 x='Recall', kde=True, bins=25, element='step', ax=axes[1], color=colormaps[i](0.6), label=class_name)\n",
    "axes[1].set_title(f'Initial Recall Distribution for each class with {Model}')\n",
    "axes[1].set_xlim(0, 1)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883b2a1",
   "metadata": {},
   "source": [
    "#### Simulated 95% CI dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop seed column in InitialPrecisionRecallDF\n",
    "InitialPrecisionRecallDF = InitialPrecisionRecallDF.drop(columns='Seed')\n",
    "\n",
    "# Make a copy of the dataframe\n",
    "InitialPrecisionRecallDF_copy = InitialPrecisionRecallDF.copy()\n",
    "\n",
    "# Create a new dataframe to store the mean precision, median recall, 50th and 950th sorted precision and recall for each class\n",
    "InitialPrecisionRecallStats = pd.DataFrame(columns=['Class', 'Mean Precision', 'Mean Recall', '0th Precision', '950th Precision', '0th Recall', '950th Recall'])\n",
    "\n",
    "# Add these metrics into the dataframe\n",
    "for i in range(4):\n",
    "    class_precision = InitialPrecisionRecallDF_copy[InitialPrecisionRecallDF_copy['Class'] == i]['Precision'].sort_values().values\n",
    "    class_recall = InitialPrecisionRecallDF_copy[InitialPrecisionRecallDF_copy['Class'] == i]['Recall'].sort_values().values\n",
    "    InitialPrecisionRecallStats.loc[i] = [\n",
    "        ['Cytosolic', 'Mitochondria', 'Nuclear', 'Secretory'][i], \n",
    "        InitialPrecisionRecallDF[InitialPrecisionRecallDF['Class'] == i]['Precision'].mean(), \n",
    "        InitialPrecisionRecallDF[InitialPrecisionRecallDF['Class'] == i]['Recall'].mean(),\n",
    "        class_precision[0],  # 0th element (index 0)\n",
    "        class_precision[949],  # 950th element (index 949)\n",
    "        class_recall[0],  # 0th element (index 0)\n",
    "        class_recall[949]  # 950th element (index 949)\n",
    "    ]\n",
    "\n",
    "InitialPrecisionRecallStats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f2367",
   "metadata": {},
   "source": [
    "#### Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b210d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "compartment_class = ['Cytosolic', 'Mitochondria', 'Nuclear', 'Secretory']\n",
    "colormaps = [cm.Blues, cm.Oranges, cm.Greens, cm.Purples]\n",
    "\n",
    "# Use a floor(X^x) to determine which epoch to plot, plot the epoch 0 and Best_epoch\n",
    "epoch_to_plot = [0] # Start with epoch 0\n",
    "for x in range(0, 100):\n",
    "    if math.floor(2**x) == 1:\n",
    "        pass\n",
    "    elif math.floor(2**x) == 2:\n",
    "        pass\n",
    "    elif math.floor(2**x) > Best_Epoch:\n",
    "        break\n",
    "    else:\n",
    "        epoch_to_plot.append(math.floor(2**x))\n",
    "\n",
    "epoch_to_plot.append(Best_Epoch) # Add the best epoch at the end\n",
    "\n",
    "# Filter out epochs in PrecisionRecallDF using the epoch_to_plot\n",
    "PrecisionRecallDF_subset = PrecisionRecallDF[PrecisionRecallDF['Epoch'].isin(epoch_to_plot)]\n",
    "\n",
    "# Read in the ci_metrics\n",
    "CI_PrecisionRecallDF = InitialPrecisionRecallStats\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    class_metrics = PrecisionRecallDF_subset[PrecisionRecallDF_subset['Class'] == i]\n",
    "    ci_metrics = CI_PrecisionRecallDF[CI_PrecisionRecallDF['Class'] == compartment_class[i]]\n",
    "\n",
    "    # Normalize epoch for color mapping\n",
    "    norm = plt.Normalize(class_metrics['Epoch'].min(), class_metrics['Epoch'].max())\n",
    "\n",
    "    # Scatter plot with gradient color based on epoch\n",
    "    for idx, row in class_metrics.iterrows():\n",
    "        color = colormaps[i](norm(row['Epoch']))\n",
    "        edge_color = colormaps[i](0.8)\n",
    "        ax.scatter(row['Recall'], row['Precision'], s=100, color=color, edgecolors=edge_color, linewidths=1)\n",
    "\n",
    "    # Connect the dots with colored lines\n",
    "    ax.plot(class_metrics['Recall'], class_metrics['Precision'], color=colormaps[i](0.5), linewidth=2)\n",
    "\n",
    "    # Plot the 0th and 950th precision and recall as shaded area\n",
    "    ax.axhline(y=ci_metrics['0th Precision'].values[0], color=colormaps[i](0.3), linestyle='--', linewidth=1)\n",
    "    ax.axhline(y=ci_metrics['950th Precision'].values[0], color=colormaps[i](0.3), linestyle='--', linewidth=1)\n",
    "    ax.axvline(x=ci_metrics['0th Recall'].values[0], color=colormaps[i](0.3), linestyle='--', linewidth=1)\n",
    "    ax.axvline(x=ci_metrics['950th Recall'].values[0], color=colormaps[i](0.3), linestyle='--', linewidth=1)\n",
    "    ax.fill_betweenx([ci_metrics['0th Precision'].values[0], ci_metrics['950th Precision'].values[0]],\n",
    "                        ci_metrics['0th Recall'].values[0], ci_metrics['950th Recall'].values[0],\n",
    "                        color=colormaps[i](0.3), alpha=0.3)\n",
    "\n",
    "    # Fill the entire area between hline and vline\n",
    "    ax.fill_betweenx([0, 1], ci_metrics['0th Recall'].values[0], ci_metrics['950th Recall'].values[0], color=colormaps[i](0.3), alpha=0.3)\n",
    "    ax.fill_between([0, 1], ci_metrics['0th Precision'].values[0], ci_metrics['950th Precision'].values[0], color=colormaps[i](0.3), alpha=0.3)\n",
    "\n",
    "    ax.set_title(compartment_class[i])\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('Recall (Sensitivity)')\n",
    "    ax.set_ylabel('Precision (Positive Predictive Value)')\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Add in a scatter dot to label the precision and recall of testing set\n",
    "    ax.scatter(J_recall[i], J_precision[i], s=100, color='red', edgecolors='red', linewidths=1, marker='s')\n",
    "    ax.scatter(M_recall[i], M_precision[i], s=100, color='black', edgecolors='black', linewidths=1, marker='s')\n",
    "    ax.scatter(K_recall[i], K_precision[i], s=100, color='blue', edgecolors='black', linewidths=1, marker='s')\n",
    "\n",
    "    # Create custom legend handles\n",
    "    krug_handle = mlines.Line2D([], [], color=colormaps[i](0.5), marker='o', linestyle='None', markersize=10, label='Krug validation set')\n",
    "    johansson_handle = mlines.Line2D([], [], color='red', marker='s', linestyle='None', markersize=10, label='Johansson test set')\n",
    "    mertins_handle = mlines.Line2D([], [], color='black', marker='s', linestyle='None', markersize=10, label='Mertins test set')\n",
    "    krug_test_handle = mlines.Line2D([], [], color='blue', marker='s', linestyle='None', markersize=10, label='Krug test set')\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend(handles=[krug_handle, johansson_handle, krug_test_handle, mertins_handle], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "figure5 = fig.savefig(figure_folder_path + f'figure5_precision_recall_{Run_Name}_{Time_Stamp}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa00903",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c99c1-5762-4620-a2d2-5040b2b7179b",
   "metadata": {},
   "source": [
    "### Summary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a5098-9243-4b22-8706-439d3edb1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Image, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.units import inch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the parameters\n",
    "parameters = {\n",
    "    'Run Name': Run_Name,\n",
    "    'Seed': Seed,\n",
    "    'Set': Set,\n",
    "    'Canvas size': Canvas_Size,\n",
    "    'Validation Fraction': Validation_Fraction,\n",
    "    'Standard deviation in add-noise transformation': StandardDeviation,\n",
    "    'Model': Model,\n",
    "    'Weight Decay (L2)': Weight_Decay,\n",
    "    'Momentum': Momentum if Optim_Name == 'SGD_weight' else 'None',\n",
    "    'Loss Function': LossFunc_Name,\n",
    "    'Optimizer': Optim_Name,\n",
    "    'Learning Rate': Learner_rate,\n",
    "    'LR Scheduler?': LrScheduler,\n",
    "    'Label Smoothing': Label_Smoothing,\n",
    "    'Class Weights': LossWeights if ApplyClassWeightToLoss else 'None',\n",
    "    'Batch Size': batch,\n",
    "    'Epoch': epochs,\n",
    "    'Best Epoch': Best_Epoch,\n",
    "    'Early stopping?': Earlystopping,\n",
    "    'Transformed?': TransformOrNot,\n",
    "    'Training sample balanced?': ImbalanceSampler,\n",
    "    'Best Validation accuracy': round(val_accuracy, 3),\n",
    "    'Validation macro-avg F1 score': round(val_f1, 3),\n",
    "    'Validation precision': np.round(val_precision, 3),\n",
    "    'Validation recall': np.round(val_recall, 3),\n",
    "    'Johansson Test accuracy with best model': round(J_test_accuracy, 3),\n",
    "    'Johansson precision': np.round(J_precision, 3),\n",
    "    'Johansson recall': np.round(J_recall, 3),\n",
    "    'Johansson F1 score': round(J_test_f1, 3),\n",
    "    'Mertins Test accuracy with best model': round(M_test_accuracy, 3),\n",
    "    'Mertins precision': np.round(M_precision, 3),\n",
    "    'Mertins recall': np.round(M_recall, 3),\n",
    "    'Mertins F1 score': round(M_test_f1, 3),\n",
    "}\n",
    "\n",
    "# Define the PDF file\n",
    "summary_file = Path(summary_folder_path+f'parameter_summary_{Run_Name}_{Time_Stamp}.pdf')\n",
    "doc = SimpleDocTemplate(str(summary_file), pagesize=letter)\n",
    "\n",
    "# Define the content\n",
    "content = []\n",
    "\n",
    "# Define a custom style with a specific font size\n",
    "styles = getSampleStyleSheet()\n",
    "custom_style = ParagraphStyle(\n",
    "    'CustomStyle',\n",
    "    parent=styles['Normal'],\n",
    "    fontSize=10,  # Specify the font size\n",
    ")\n",
    "caption_style = ParagraphStyle(\n",
    "    'CaptionStyle',\n",
    "    parent=styles['Normal'],\n",
    "    fontSize=10,  # Specify the font size for captions\n",
    ")\n",
    "\n",
    "# Add the parameter summary to the content\n",
    "content.append(Paragraph('Parameter Summary:', styles['Title']))\n",
    "for key, value in parameters.items():\n",
    "    content.append(Paragraph(f'{key}: {value}', custom_style))\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "# Save the figures and include them in the content\n",
    "figure1.savefig(Path(figure_folder_path+f'figure1_loss_{Run_Name}_{Time_Stamp}.png'), dpi=300, bbox_inches='tight')\n",
    "figure2.savefig(Path(figure_folder_path+f'figure2_accuracy_{Run_Name}_{Time_Stamp}.png'), dpi=300, bbox_inches='tight')\n",
    "content.append(Paragraph('Figures:', styles['Title']))\n",
    "\n",
    "img1 = Image(Path(figure_folder_path+f'figure1_loss_{Run_Name}_{Time_Stamp}.png'))\n",
    "img1.drawHeight = 6*inch*img1.drawHeight / img1.drawWidth\n",
    "img1.drawWidth = 6*inch\n",
    "content.append(img1)\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "img2 = Image(Path(figure_folder_path+f'figure2_accuracy_{Run_Name}_{Time_Stamp}.png'))\n",
    "img2.drawHeight = 6*inch*img2.drawHeight / img2.drawWidth\n",
    "img2.drawWidth = 6*inch\n",
    "content.append(img2)\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "img3 = Image(Path(figure_folder_path+f'figure3_val_cm_best_model_{Run_Name}_{Time_Stamp}.png'))\n",
    "img3.drawHeight = 6*inch*img3.drawHeight / img3.drawWidth\n",
    "img3.drawWidth = 6*inch\n",
    "content.append(img3)\n",
    "content.append(Spacer(1, 0.2 * inch))  # Add a blank line\n",
    "\n",
    "img5 = Image(Path(figure_folder_path+f'figure5_precision_recall_{Run_Name}_{Time_Stamp}.png'))\n",
    "img5.drawHeight = 6*inch*img5.drawHeight / img5.drawWidth\n",
    "img5.drawWidth = 6*inch\n",
    "content.append(img5)\n",
    "\n",
    "# Build the PDF\n",
    "doc.build(content)\n",
    "\n",
    "# Print a message indicating that the parameter summary and figures have been saved\n",
    "print('Parameter summary and figures have been saved.')\n",
    "display(HTML('<a href=\"{}\" target=\"_blank\">Click here to download {}</a>'.format(summary_file, summary_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e51dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataframes\n",
    "PerformanceDF.to_excel(Path(performance_folder_path + f'PerformanceDF_{Time_Stamp}.xlsx'), index=False)\n",
    "PrecisionRecallDF.to_excel(Path(performance_folder_path + f'PrecisionRecallDF_{Time_Stamp}.xlsx'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
